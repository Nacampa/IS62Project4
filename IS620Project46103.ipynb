{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load assignment12v32.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# IS620 - Assignment 12 (6.10.3)\n",
      "# Program: assignment12.py\n",
      "# Student: Neil Acampa\n",
      "# Date:    10/31/16\n",
      "# Function:\n",
      "\n",
      "#    6.10.3   Word sense disambiquation\n",
      "#    Prompt for keyword\n",
      "#    Read Alice and wonderland and parse sentences with keyword\n",
      "#    Store in sentword array\n",
      "#    Find keyword in sentence with 2 prior words and 2 words after keyword\n",
      "#    Store in sentwordadj array\n",
      "#\n",
      "#    Using the keyword and the wordnet update the syns and synsdef arrrays\n",
      "#\n",
      "#    Create features using the Lesk algorithm\n",
      "#    For a given word and sentence:                                                   \n",
      "#    The Lesk algorithm returns a Synset with the highest number of overlapping words\n",
      "#    between the context sentence and different definitions from each Synset. \n",
      "#    The feature set shows the Snyset and Synset definition   \n",
      "#    Display the features   \n",
      "#\n",
      "#    Evaluate features pos/neg score\n",
      "\n",
      "\n",
      "from __future__ import absolute_import \n",
      "from __future__ import division\n",
      "import re\n",
      "import os \n",
      "import math\n",
      "import decimal\n",
      "import numpy as np\n",
      "import scipy\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "from pandas import DataFrame\n",
      "import networkx as nx\n",
      "import random\n",
      "from urllib import urlopen\n",
      "import nltk\n",
      "nltk.download('gutenberg')\n",
      "from nltk import word_tokenize\n",
      "nltk.download('maxent_treebank_pos_tagger')\n",
      "nltk.download('punkt')\n",
      "nltk.download('movie_reviews')\n",
      "nltk.download('senseval')\n",
      "nltk.download('wordnet')\n",
      "nltk.download('sentiwordnet')\n",
      "nltk.download('lesk')\n",
      "nltk.download('stopwords')\n",
      "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
      "from nltk.corpus import movie_reviews\n",
      "from nltk.corpus import stopwords\n",
      "stopwords = nltk.corpus.stopwords.words('english')\n",
      "from nltk.corpus import senseval\n",
      "from nltk.corpus import wordnet as wn\n",
      "from nltk.wsd import lesk\n",
      "from nltk.corpus import sentiwordnet as swn\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "linelst=[]\n",
      "lines  = \"\"\n",
      "allwords          = []   # Contains all words\n",
      "\n",
      " \n",
      "sentword          = []   # Holds an entire sentence with comma separted words\n",
      "sentwordadj       = []   # Holds a sentence with the 2 prior and 2 words after the keyword\n",
      "\n",
      "syns              = []   # Synset id for a specific word \n",
      "synsdef           = []   # The corresponding Synset Sentence\n",
      "synsname          = []   # The corresponding Synset name (i.e. 'look.v.09')\n",
      "\n",
      "results           = []\n",
      "# Table Elements\n",
      "\n",
      "fheadings      = [] \n",
      "\n",
      "\n",
      "fheadings.append(\"Train on 90%, Test on first 10%\")\n",
      "\n",
      "\n",
      "rejectchars = [',','.','?','<','>','!','\"','-','%','&','#','(',')','*',';'];\n",
      "rcnt = len(rejectchars);\n",
      "\n",
      "\n",
      "def remove_characters(word):\n",
      "  \"\"\"Replace special characters in the word\"\"\"\n",
      "  \n",
      "  for i in range(rcnt):\n",
      "    rchar = rejectchars[i]\n",
      "    if rchar in word:\n",
      "      word = word.replace(rchar,\"\")\n",
      "\n",
      "  return word\n",
      "\n",
      "\n",
      "def remove_symbols(word):\n",
      "  \"\"\"Replace symbols in the word\"\"\"\n",
      "  w = len(word)\n",
      "  word = (ord(c) for c in word) \n",
      "  word = map(lambda x:x if x<123 or x>255 else \" \", word)\n",
      "  newword=\"\"\n",
      "  for c in range(w):\n",
      "    if word[c] <> \" \":\n",
      "      newword += chr(word[c]);\n",
      "  \n",
      "  return newword\n",
      "\n",
      "\n",
      "def find_word(word, sentence):\n",
      "  \"\"\"Find and return index of word in sentence\"\"\"\n",
      "\n",
      "  masterlen = len(sentence)\n",
      "  find=0\n",
      "  temp=\"x\"\n",
      "  try:\n",
      "   temp = sentence.index(word);\n",
      "   return temp\n",
      "  except ValueError:\n",
      "   return temp\n",
      "\n",
      "\n",
      "def find_phrase(phrase, syns):\n",
      "  \"\"\"Find and return index of Synset in sentence\"\"\"\n",
      "\n",
      "  masterlen = len(syns)\n",
      "  find=0\n",
      "  temp=\"x\"\n",
      "  try:\n",
      "   temp = syns.index(phrase);\n",
      "   return temp\n",
      "  except ValueError:\n",
      "   return temp\n",
      "\n",
      "\n",
      "\n",
      "def document_features(sentence, keyword):\n",
      "  \"\"\" For a given word and sentence                                                    \"\"\"\n",
      "  \"\"\" The Lesk algorithm returns a Synset with the highest number of overlapping words \"\"\"\n",
      "  \"\"\" between the context sentence and different definitions from each Synset.         \"\"\"\n",
      "\n",
      "  features = {}\n",
      "  synsent  = \"\"\n",
      "  phrase   = lesk(sentence,keyword)\n",
      "  findx = find_phrase(phrase, syns)\n",
      "  if (findx != \"x\"):\n",
      "    synsent = synsdef[findx]\n",
      "    synsent = synsent.encode('ascii')\n",
      "   \n",
      " \n",
      "  features['(%s : %s)' % (sentence, phrase)]  = synsent\n",
      "  \n",
      "  return features\n",
      "\n",
      "def document_features1(sentence, keyword):\n",
      "  \"\"\" For a given word and sentence                                                    \"\"\"\n",
      "  \"\"\" The Lesk algorithm returns a Synset with the highest number of overlapping words \"\"\"\n",
      "  \"\"\" between the context sentence and different definitions from each Synset.         \"\"\"\n",
      "\n",
      "  features = {}\n",
      "  synsent  = \"\"\n",
      "  phrase   = lesk(sentence,keyword)\n",
      "  findx = find_phrase(phrase, syns)\n",
      "  s = \"\"\n",
      "  results1 = \"\"\n",
      "  if (findx != \"x\"):\n",
      "    s = synsname[findx]\n",
      "    synsent = synsdef[findx]\n",
      "    synsent = synsent.encode('ascii')\n",
      "    results1 = swn.senti_synset(s)\n",
      "    temp = (\"%s %s\")  % (keyword,results1)\n",
      "    features['(%s : %s)' % (keyword, phrase)]  = results1\n",
      "    \n",
      "  \n",
      "  return features\n",
      "\n",
      "\n",
      "\n",
      "print\n",
      "print\n",
      "print\n",
      "defaultword = 'look'\n",
      "keyword = raw_input(\"Please enter an ambiguous word in Alice in Wonderland \")\n",
      "print (\"or Press return to use the default word %s\") % (defaultword)\n",
      "valid = 0\n",
      "if keyword == \"\":\n",
      "     keyword =  defaultword\n",
      "\n",
      "print\n",
      "print\n",
      "\n",
      "\n",
      "sentences = \"\"\n",
      "sentword  = []\n",
      "corpus     = \"Alice in Wonderland\"\n",
      "fullcorpus = \"Alice in Wonderland by Lewis Carroll\"\n",
      "cwd = os.getcwd()\n",
      "currfilepath = str(cwd) + \"\\carroll-alice.txt\"\n",
      "print currfilepath\n",
      "print (\"Enter the Full File Path including the File\")\n",
      "print (\"or Press return to use current File Path %s\") % (currfilepath)\n",
      "filepath = raw_input(\"Please enter the File Path now \")\n",
      "valid = 0\n",
      "if filepath == \"\":\n",
      "     filepath = currfilepath\n",
      "\n",
      " \n",
      "try:\n",
      "       f = open(filepath,\"r\")\n",
      "       try:\n",
      "         valid=1\n",
      "         x =0\n",
      "         j=0\n",
      "         findword =0\n",
      "         for lines in f:\n",
      "           lines = lines.rstrip()\n",
      "           temp = lines.split(\" \");\n",
      "           l = len(temp)\n",
      "           senttemp = \"\"\n",
      "           for x in range(l):\n",
      "             word = remove_characters(temp[x])\n",
      "             word = remove_symbols(word)\n",
      "             word = word.lower()\n",
      "             word = word.replace(\" \",\"\")\n",
      "             if (word != ''):\n",
      "               if (word == keyword):\n",
      "                 findword=1\n",
      "\n",
      "               allwords.append(word)\n",
      "               if senttemp == \"\":\n",
      "                 senttemp = word\n",
      "               else:\n",
      "                 senttemp = senttemp + \",\" + word \n",
      "           \n",
      "           if (findword == 1):\n",
      "               sentword.append(senttemp) \n",
      "\n",
      "           findword = 0        \n",
      "       finally:\n",
      "            f.close()\n",
      "         \n",
      "except IOError:\n",
      "       print (\"File not Found - Program aborting\")\n",
      "\n",
      "if not(valid):\n",
      "   exit();\n",
      "\n",
      "\n",
      "\n",
      "# Trim sentence and use 2 words before and after key word\n",
      "print\n",
      "print(\"Trimming Sentences in %s\") % (corpus)\n",
      "indx = 0\n",
      "sl = len(sentword)\n",
      "for i in range(sl):\n",
      "   l = len(sentword[i])\n",
      "   s = sentword[i].split(\",\")\n",
      "   #s = s.split(\",\")\n",
      "   indx = s.index(keyword)\n",
      "   start = 0\n",
      "   fin   = l\n",
      "   if ((indx > 1) and (indx < l)):\n",
      "     start = indx - 2\n",
      "     fin   = indx + 3\n",
      "   else:\n",
      "     if (indx <= 1):\n",
      "       start = 0\n",
      "     if (indx >= l-1):\n",
      "       fin = l\n",
      "\n",
      "   s1 = s[start:fin]\n",
      "   sentwordadj.append(s1)\n",
      "\n",
      "sla = len(sentwordadj)\n",
      "\n",
      "\n",
      "\n",
      "# Store Synset defintions for keyword \n",
      "print\n",
      "print\n",
      "print(\"Synsets for Keyword: %s\") % (keyword)\n",
      "print\n",
      "for ss in wn.synsets(keyword):\n",
      "    syns.append(ss)\n",
      "    synsdef.append(ss.definition())\n",
      "    temp = ss.name()\n",
      "    temp = temp.encode('ascii')\n",
      "    synsname.append(temp)\n",
      "    print(\"%s\\t%s\") % (ss,ss.definition())\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "# Get Sysnet features for each trimmed sentence\n",
      "print\n",
      "print\n",
      "print(\"Getting feature set for each trimmed sentence\")\n",
      "featuresets=[]\n",
      "for i in range(sla):\n",
      "  featuresets.append(document_features(sentwordadj[i], keyword))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "print(\"Corpus: %s  %i instances of KeyWord: %s\") % (fullcorpus,sl,keyword)\n",
      "print\n",
      "print(\"Sentence\\tSense Tag\\tSense Tag Description\")\n",
      "print\n",
      "for i in range(sla):\n",
      "  print\n",
      "  print featuresets[i]\n",
      "\n",
      "\n",
      "\n",
      "print\n",
      "print(\"Getting Sentiment Value for each trimmed sentence\")\n",
      "print\n",
      "featuresets1=[]\n",
      "for i in range(sla):\n",
      "  phrase   = lesk(sentwordadj[i],keyword)\n",
      "  findx    = find_phrase(phrase, syns)\n",
      "  s = \"\"\n",
      "  results = \"\"\n",
      "  if (findx != \"x\"):\n",
      "    s = synsname[findx]\n",
      "    synsent = synsdef[findx]\n",
      "    synsent = synsent.encode('ascii')\n",
      "    results1 = swn.senti_synset(s)\n",
      "    pscore = results1.pos_score()\n",
      "    nscore = results1.neg_score()\n",
      "    temp = (\"%s\\t%.2f\\t%.2f\")  % (syns[findx],pscore,nscore)\n",
      "    featuresets1.append(temp)\n",
      "    \n",
      "\n",
      "\n",
      "print\n",
      "print\n",
      "print(\"Corpus: %s  %i instances of KeyWord: %s\") % (fullcorpus,sl,keyword)\n",
      "print(\"Sentiment Scores based on Word Sense Definitions\")\n",
      "print\n",
      "print(\"Sense Tag       \\tPos\\tNeg\")\n",
      "print(\"                \\tScore\\tScore\")\n",
      "print\n",
      "for i in range(sla):\n",
      "  print\n",
      "  print featuresets1[i]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "l = len(featuresets1)\n",
      "print\n",
      "# Train on 90%, Test on 10%\n",
      "testlim  = int(l *.01)\n",
      "train_set, test_set  = featuresets[testlim:], featuresets[:testlim]\n",
      "#classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "#accuracy   = nltk.classify.accuracy(classifier, test_set)\n",
      "#results.append(accuracy)\n",
      "#print(classifier.show_most_informative_features(5))\n",
      "\n",
      "print\n",
      "print\n",
      "print\n",
      "#l = len(results)\n",
      "#print (\"%s\\t%s\\t%s\") % (\"Feature\" , \"Feature Desc                              \", \"                Accuracy\")\n",
      "#indx = 0\n",
      "#for i in range(l):\n",
      "  #indx = indx + 1\n",
      "  #print(\"%d\\t%s\\t%.4f\") % (indx, fheadings[i], results[i])\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run assignment12v32.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[nltk_data] Downloading package gutenberg to\n",
        "[nltk_data]     C:\\Users\\nacampa\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package gutenberg is already up-to-date!\n",
        "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
        "[nltk_data]     C:\\Users\\nacampa\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
        "[nltk_data]       date!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data] Downloading package punkt to\n",
        "[nltk_data]     C:\\Users\\nacampa\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package punkt is already up-to-date!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data] Downloading package movie_reviews to\n",
        "[nltk_data]     C:\\Users\\nacampa\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package movie_reviews is already up-to-date!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data] Downloading package senseval to\n",
        "[nltk_data]     C:\\Users\\nacampa\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package senseval is already up-to-date!\n",
        "[nltk_data] Downloading package wordnet to\n",
        "[nltk_data]     C:\\Users\\nacampa\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package wordnet is already up-to-date!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[nltk_data] Downloading package sentiwordnet to\n",
        "[nltk_data]     C:\\Users\\nacampa\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
        "[nltk_data] Error loading lesk: Package 'lesk' not found in index\n",
        "[nltk_data] Downloading package stopwords to\n",
        "[nltk_data]     C:\\Users\\nacampa\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package stopwords is already up-to-date!\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Please enter an ambiguous word in Alice in Wonderland \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "or Press return to use the default word look\n",
        "\n",
        "\n",
        "C:\\Anaconda2\\carroll-alice.txt\n",
        "Enter the Full File Path including the File\n",
        "or Press return to use current File Path C:\\Anaconda2\\carroll-alice.txt\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Please enter the File Path now \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Trimming Sentences in Alice in Wonderland\n",
        "\n",
        "\n",
        "Synsets for Keyword: look\n",
        "\n",
        "Synset('expression.n.01')\tthe feelings expressed on a person's face"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Synset('look.n.02')\tthe act of directing the eyes toward something and perceiving it visually\n",
        "Synset('look.n.03')\tphysical appearance\n",
        "Synset('spirit.n.02')\tthe general atmosphere of a place or situation and the effect that it has on people\n",
        "Synset('look.v.01')\tperceive with attention; direct one's gaze towards\n",
        "Synset('look.v.02')\tgive a certain impression or have a certain outward aspect\n",
        "Synset('look.v.03')\thave a certain outward or facial expression\n",
        "Synset('search.v.02')\tsearch or seek\n",
        "Synset('front.v.01')\tbe oriented in a certain direction, often with respect to another reference point; be opposite to\n",
        "Synset('attend.v.02')\ttake charge of or deal with\n",
        "Synset('look.v.07')\tconvey by one's expression\n",
        "Synset('expect.v.03')\tlook forward to the probable occurrence of\n",
        "Synset('look.v.09')\taccord in appearance with\n",
        "Synset('count.v.08')\thave faith or confidence in\n",
        "\n",
        "\n",
        "Getting feature set for each trimmed sentence\n",
        "Corpus: Alice in Wonderland by Lewis Carroll  27 instances of KeyWord: look\n",
        "\n",
        "Sentence\tSense Tag\tSense Tag Description\n",
        "\n",
        "\n",
        "{\"(['down', 'to', 'look', 'about', 'her'] : Synset('expect.v.03'))\": 'look forward to the probable occurrence of'}\n",
        "\n",
        "{\"(['tried', 'to', 'look', 'down', 'and'] : Synset('expect.v.03'))\": 'look forward to the probable occurrence of'}\n",
        "\n",
        "{'([\"\\'no\", \"i\\'ll\", \\'look\\', \"first\\'\", \\'she\\'] : Synset(\\'expect.v.03\\'))': 'look forward to the probable occurrence of'}\n",
        "\n",
        "{\"(['directions', 'will', 'look'] : Synset('expect.v.03'))\": 'look forward to the probable occurrence of'}\n",
        "\n",
        "{\"(['look', 'through', 'into', 'the', 'garden', 'with', 'one', 'eye', 'but', 'to', 'get', 'through', 'was', 'more'] : Synset('expect.v.03'))\": 'look forward to the probable occurrence of'}\n",
        "\n",
        "{\"(['shall', 'only', 'look', 'up', 'and'] : Synset('spirit.n.02'))\": 'the general atmosphere of a place or situation and the effect that it has on people'}\n",
        "\n",
        "{'([\\'soon\\', \\'as\\', \\'look\\', \\'at\\', \"it\\'\"] : Synset(\\'expect.v.03\\'))': 'look forward to the probable occurrence of'}\n",
        "\n",
        "{\"(['coming', 'to', 'look', 'for', 'her'] : Synset('expect.v.03'))\": 'look forward to the probable occurrence of'}\n",
        "\n",
        "{\"(['made', 'her', 'look', 'up', 'in'] : Synset('look.v.09'))\": 'accord in appearance with'}\n",
        "\n",
        "{\"(['as', 'well', 'look', 'and', 'see'] : Synset('spirit.n.02'))\": 'the general atmosphere of a place or situation and the effect that it has on people'}\n",
        "\n",
        "{\"(['like', 'the', 'look', 'of', 'the'] : Synset('expect.v.03'))\": 'look forward to the probable occurrence of'}\n",
        "\n",
        "{\"(['round', 'and', 'look', 'up', 'in'] : Synset('spirit.n.02'))\": 'the general atmosphere of a place or situation and the effect that it has on people'}\n",
        "\n",
        "{\"(['like', 'the', 'look', 'of', 'it'] : Synset('spirit.n.02'))\": 'the general atmosphere of a place or situation and the effect that it has on people'}\n",
        "\n",
        "{'([\"\\'and\", \"don\\'t\", \\'look\\', \\'at\\', \\'me\\'] : Synset(\\'expect.v.03\\'))': 'look forward to the probable occurrence of'}\n",
        "\n",
        "{\"(['cat', 'may', 'look', 'at', 'a'] : Synset('spirit.n.02'))\": 'the general atmosphere of a place or situation and the effect that it has on people'}\n",
        "\n",
        "{\"(['like', 'the', 'look'] : Synset('expect.v.03'))\": 'look forward to the probable occurrence of'}\n",
        "\n",
        "{\"(['whole', 'party', 'look', 'so', 'grave'] : Synset('expect.v.03'))\": 'look forward to the probable occurrence of'}\n",
        "\n",
        "{'([\\'it\\', \"doesn\\'t\", \\'look\\', \\'like\\', \\'one\\'] : Synset(\\'spirit.n.02\\'))': 'the general atmosphere of a place or situation and the effect that it has on people'}\n",
        "\n",
        "{\"(['gryphon', 'is', 'look', 'at', 'the'] : Synset('expect.v.03'))\": 'look forward to the probable occurrence of'}\n",
        "\n",
        "{\"(['like', 'the', 'look', 'of', 'the'] : Synset('expect.v.03'))\": 'look forward to the probable occurrence of'}\n",
        "\n",
        "{\"(['gave', 'a', 'look', 'askance'] : Synset('spirit.n.02'))\": 'the general atmosphere of a place or situation and the effect that it has on people'}\n",
        "\n",
        "{'([\\'hungry\\', \\'to\\', \\'look\\', \\'at\\', \"them\\'i\"] : Synset(\\'expect.v.03\\'))': 'look forward to the probable occurrence of'}\n",
        "\n",
        "{\"(['wig', 'look', 'at', 'the', 'frontispiece', 'if', 'you', 'want', 'to', 'see', 'how', 'he', 'did', 'it', 'he', 'did'] : Synset('expect.v.03'))\": 'look forward to the probable occurrence of'}\n",
        "\n",
        "{\"(['not', 'look', 'at', 'all', 'comfortable', 'and', 'it', 'was', 'certainly', 'not', 'becoming'] : Synset('spirit.n.02'))\": 'the general atmosphere of a place or situation and the effect that it has on people'}\n",
        "\n",
        "{\"(['an', 'anxious', 'look', 'at', 'the'] : Synset('expect.v.03'))\": 'look forward to the probable occurrence of'}\n",
        "\n",
        "{\"(['enough', 'to', 'look', 'over', 'their'] : Synset('expect.v.03'))\": 'look forward to the probable occurrence of'}\n",
        "\n",
        "{'([\"\\'do\", \\'i\\', \\'look\\', \\'like\\', \"it\\'\"] : Synset(\\'expect.v.03\\'))': 'look forward to the probable occurrence of'}\n",
        "\n",
        "Getting Sentiment Value for each trimmed sentence\n",
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Corpus: Alice in Wonderland by Lewis Carroll  27 instances of KeyWord: look\n",
        "Sentiment Scores based on Word Sense Definitions\n",
        "\n",
        "Sense Tag       \tPos\tNeg\n",
        "                \tScore\tScore\n",
        "\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('spirit.n.02')\t0.00\t0.00\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('look.v.09')\t0.38\t0.00\n",
        "\n",
        "Synset('spirit.n.02')\t0.00\t0.00\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('spirit.n.02')\t0.00\t0.00\n",
        "\n",
        "Synset('spirit.n.02')\t0.00\t0.00\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('spirit.n.02')\t0.00\t0.00\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('spirit.n.02')\t0.00\t0.00\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('spirit.n.02')\t0.00\t0.00\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('spirit.n.02')\t0.00\t0.00\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "Synset('expect.v.03')\t0.00\t0.25\n",
        "\n",
        "\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}